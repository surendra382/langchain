{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to LangChain\n",
        "\n",
        "- Using language models\n",
        "- Using PromptTemplates and OutputParsers\n",
        "- Using LangChain Expression Language (LCEL) to chain components together\n",
        "- Debugging and tracing your application using LangSmith"
      ],
      "metadata": {
        "id": "gVoYRIWMTehQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "Some providers come in their own package, such as groq and openai."
      ],
      "metadata": {
        "id": "AlsmTZ0VaiP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iR4b21s9TLj8",
        "outputId": "30757e99-1804-4c63-f0a5-04c3a09ec58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.7 (from langchain)\n",
            "  Downloading langchain_core-0.2.8-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.79-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.7->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.5 langchain-core-0.2.8 langchain-text-splitters-0.2.1 langsmith-0.1.79 orjson-3.10.5\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-openai\n",
        "%pip install -qU langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTYjV8EBTdt4",
        "outputId": "9316592a-191a-4f25-a8b3-0886d1a53f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnables\n",
        "\n",
        "`ChatOpenAI`is an instance of a `Runnable` in Langchain. It is similar to the client we created in the previous version, but it is much more flexible.\n",
        "\n",
        "A `Runnable` is an object that exposes a standard interface for interacting with it.\n",
        "\n",
        "This means that `Runnables` have the same methods that can be called. This makes working with the framework much more flexible since they allow you to work with several service providers."
      ],
      "metadata": {
        "id": "h-QmdXaRVn5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model_openai = ChatOpenAI() # model defaults to 'gpt-3.5-turbo'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGK7N5MWUBDc",
        "outputId": "06ac2466-50aa-49d3-8793-55adf595260b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model_groq = ChatGroq(model=\"llama3-8b-8192\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMwFHsTmW3p-",
        "outputId": "0984eb38-09cf-4ead-9ae6-570abe30a826"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from French into English\"),\n",
        "    HumanMessage(content=\"Bonjour ! Il fait très beau à Paris aujourd'hui.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "xyVH8JxcUZM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_openai = model_openai.invoke(messages)\n",
        "\n",
        "response_groq = model_groq.invoke(messages)"
      ],
      "metadata": {
        "id": "rGNSrhVoXJ_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owO40ln_X3EY",
        "outputId": "e8de9636-5803-4ade-a1c5-65f033e80d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! The weather is very nice in Paris today.', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 29, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c8ae9b50-52fb-4416-b468-607a9e2cc4c9-0', usage_metadata={'input_tokens': 29, 'output_tokens': 11, 'total_tokens': 40})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfiUiz3VX4QM",
        "outputId": "9e6beda2-7322-4e72-836a-b3a83f4457e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Here is the translation:\\n\\n\"Hello! It\\'s very nice in Paris today.\"', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 33, 'total_tokens': 50, 'completion_time': 0.012881826, 'prompt_time': 0.005488198, 'queue_time': None, 'total_time': 0.018370024}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_af05557ca2', 'finish_reason': 'stop', 'logprobs': None}, id='run-f5c87ad9-b570-4629-902b-e109223ee6ca-0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DsDE_8tAX_It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parsers\n",
        "\n",
        "In a process, a LLM call will only be one of the steps that compose it. So it is a good idea to always have a consistent data type as a result.\n",
        "\n",
        "In this case, we are getting an `AIMessage` instance as a result from calling our model. Let's use `StrOutputParser`, an Output Parser to makes sure your result is always a string."
      ],
      "metadata": {
        "id": "OMGrBr5YYKGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "pC-zac1hYxvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser.invoke(response_groq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E-ur7KIJY4y-",
        "outputId": "5cafe988-b162-4a7d-acea-b9ba8ea60fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is the translation:\\n\\n\"Hello! It\\'s very nice in Paris today.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.invoke(response_openai)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nddsGE_7ZHDw",
        "outputId": "6d63ade3-181c-4c19-a3ea-b9f48578b3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! The weather is very nice in Paris today.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gIduOCkHZIms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains\n",
        "\n",
        "We can now chain these two components together. This way we can have the output parser be applied to the model call every single time.\n",
        "\n",
        "To do this, we can create a `chain`.\n",
        "\n",
        "Similarly to bash, chains in LangChain are created using the `|` operator."
      ],
      "metadata": {
        "id": "Pt-f5Gx6ZaiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_groq | parser\n",
        "\n",
        "chain = model | parser"
      ],
      "metadata": {
        "id": "94SZUfKYZmCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it exposes the same interface as the model and parser\n",
        "\n",
        "chain.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B15jAbgdZ6-2",
        "outputId": "97d2a85e-6b39-4573-d7bf-a19e248cd830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here\\'s the translation:\\n\\n\"Hello! It\\'s very beautiful in Paris today.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates\n",
        "\n",
        "Prompt templates allow you to streamline a consistent prompt that you will send to your LLM. You will only need to specify in your call the variables that you want to change."
      ],
      "metadata": {
        "id": "i3bMQp_CbiJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt templates\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})"
      ],
      "metadata": {
        "id": "VZyI3LXewSYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat prompt templates\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Translate the following into {language}:\"),\n",
        "        (\"user\", \"{text}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "mgjStDROaCYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-TCQTaBcNTB",
        "outputId": "f39a978a-c554-4fd2-d061-59a1576de6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following into italian:'), HumanMessage(content='hi')])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    MessagesPlaceholder(\"msgs\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
      ],
      "metadata": {
        "id": "d1Ww9KHl2xKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a `ChatPromptValue`. Let's turn that into messages:"
      ],
      "metadata": {
        "id": "TkMoRyoXcp6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIUoujitcPB0",
        "outputId": "8742d5d4-10eb-476f-b9bf-50b2c749729b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following into italian:'),\n",
              " HumanMessage(content='hi')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chaining all together"
      ],
      "metadata": {
        "id": "Y4xnylhGc1oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | model | parser"
      ],
      "metadata": {
        "id": "gUASXOKCco-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"language\": \"french\", \"text\": \"Hello! This is a translated sentence.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lbz9ujmOc5_M",
        "outputId": "775a4344-384e-4de4-f7fd-fecb265ed7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bonjour ! Voici une phrase traduite.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFsHR1ybdAxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More on Runnables\n",
        "\n",
        "We will see streaming in detail later. But for now, here is a sneak peak of what streaming looks like in LangChain."
      ],
      "metadata": {
        "id": "NPxTwdvvhflo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")\n",
        "parser = StrOutputParser()\n",
        "chain = prompt | model_groq | parser\n",
        "\n",
        "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e97yFrEKhiWP",
        "outputId": "c4644af4-7e3e-4286-c5a7-8ddbc1285e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a story about a colorful and charming parrot:\n",
            "\n",
            "In a lush jungle, surrounded by vibrant flowers and towering trees, there lived a majestic parrot named Sunny. Sunny was a brilliant shade of blue and green, with an iridescent sheen that shimmered in the sunlight. Her feathers were so bright that they seemed to glow from within, making her a beacon of joy and beauty in the jungle.\n",
            "\n",
            "Sunny loved to fly through the jungle, exploring every nook and cranny, and making friends with all the creatures she met along the way. She was a gentle soul, with a kind heart and a quick wit. Her best friend was a wise old owl named Professor Hootenanny, who lived in a hollow tree near the jungle's central clearing.\n",
            "\n",
            "One day, a severe storm rolled in over the jungle, bringing heavy rains and strong winds. The creatures of the jungle huddled together, frightened by the loud thunder and flashes of lightning. But Sunny, being the brave parrot she was, decided to take matters into her own beak. She flew out into the storm, determined to find a way to calm the frightened animals and restore peace to the jungle.\n",
            "\n",
            "As she flew over the clearing, Sunny spotted a group of baby monkeys huddled together, trembling with fear. She swooped down, her bright feathers glowing in the dim light, and began to sing a gentle melody. Her song was like nothing anyone had ever heard before – a soothing harmony that seemed to match the rhythm of the raindrops.\n",
            "\n",
            "The baby monkeys, entranced by Sunny's song, slowly began to calm down. They looked up at her with wide, trusting eyes, and Sunny knew she had to keep singing. She flew over to the other animals, singing her song to each and every one, until the entire jungle was filled with the sweet sound of her melody.\n",
            "\n",
            "As the storm began to subside, the creatures of the jungle emerged from their hiding places, blinking in the bright sunlight. They looked around at the destruction caused by the storm, but they also looked at each other with gratitude. For in the midst of the chaos, Sunny's song had brought them comfort and peace.\n",
            "\n",
            "From that day on, Sunny was hailed as the hero of the jungle. The creatures celebrated her bravery and kindness, and she became known as the parrot with the healing song. And whenever the jungle needed cheering up, Sunny would take to the skies, her bright feathers shining like a beacon of hope, and her beautiful song filling the air.\n",
            "\n",
            "How was that? Would you like me to add more to the story or create a new one for you?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I99-7iy9iYjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}